# -*- coding: utf-8 -*-
"""llm1Fyp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18ol51JqF6BlOS1P4qhC9PCJj8K2COZTP

**bold text**<!-- Codes by HTMLcodes.ws -->
<h1 style = "color:Blue;font-family:newtimeroman;font-size:250%;text-align:center;border-radius:15px 50px;">Chatbot for Mental Health Conversations</h1>

# Introduction

Building chatbots capable of providing emotional support to individuals experiencing anxiety and depression has become a key focus in the field of artificial intelligence. A crucial component in developing such chatbots is a well-structured dataset, which serves as the foundation for training models to comprehend and respond empathetically to user messages.

The dataset available here is a comprehensive collection of conversations related to mental health. It encompasses various conversation types, including basic exchanges, frequently asked questions about mental health, classical therapy discussions, and general advice given to individuals facing anxiety and depression. The primary objective of this dataset is to facilitate the training of a chatbot model that emulates a therapist, capable of providing empathetic and supportive responses to those seeking emotional solace.

To train the model effectively, the dataset incorporates the concept of "intents." Each intent represents the underlying purpose behind a user's message. For example, if a user expresses sadness, the associated intent would be "sad." Each intent is accompanied by a set of patterns, which are example messages aligning with the specific intent, as well as corresponding responses that the chatbot should generate based on that intent. Through defining multiple intents and their respective patterns and responses, the model learns to identify user intents and generate relevant and compassionate replies.

By utilizing this dataset, researchers and developers can train chatbot models to better understand and support individuals coping with anxiety and depression. The goal is to create a virtual conversational agent that can offer emotional guidance, provide helpful insights, and alleviate some of the challenges faced by those seeking mental health support.

# Data Preparation

* Load the dataset into a suitable data structure (e.g., Pandas DataFrame).
* Examine the dataset to understand its structure and distribution.
* Preprocess the data by removing unnecessary characters, converting text to lowercase, and handling any missing values.
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
df = pd.read_json("/content/intents_merged_unique.json")

"""## Explore Input Data

### Subtask:
List the files available in the input directory to understand the dataset structure.
"""

import json

with open('/content/intents_merged_unique.json', 'r') as f:
    data = json.load(f)

df = pd.DataFrame(data['intents'])
df

dic = {"tag":[], "patterns":[], "responses":[]}
for i in range(len(df)):
    ptrns = df[df.index == i]['patterns'].values[0]
    rspns = df[df.index == i]['responses'].values[0]
    tag = df[df.index == i]['tag'].values[0]
    for j in range(len(ptrns)):
        dic['tag'].append(tag)
        dic['patterns'].append(ptrns[j])
        dic['responses'].append(rspns)

df = pd.DataFrame.from_dict(dic)
df

from google.colab import drive
drive.mount('/content/drive')

df['tag'].unique()

"""# Exploratory Data Analysis

* Analyze the distribution of intents in the dataset.
* Visualize the frequency of different intents using a bar plot from the Plotly library. The x-axis can represent the intents, and the y-axis can represent the count of patterns or responses associated with each intent.


"""

import plotly.graph_objects as go

intent_counts = df['tag'].value_counts()
fig = go.Figure(data=[go.Bar(x=intent_counts.index, y=intent_counts.values)])
fig.update_layout(title='Distribution of Intents', xaxis_title='Intents', yaxis_title='Count')
fig.show()

"""# Pattern and Response Analysis

* Explore the patterns and responses associated with each intent.
* Calculate the average number of patterns and responses per intent.
* Visualize this information using a Plotly bar plot, where the x-axis represents the intents, and the y-axis represents the average count of patterns or responses.
* Interpret the plot to understand the varying degrees of complexity and diversity in patterns and responses across different intents.
"""

df['pattern_count'] = df['patterns'].apply(lambda x: len(x))
df['response_count'] = df['responses'].apply(lambda x: len(x))
avg_pattern_count = df.groupby('tag')['pattern_count'].mean()
avg_response_count = df.groupby('tag')['response_count'].mean()

fig = go.Figure()
fig.add_trace(go.Bar(x=avg_pattern_count.index, y=avg_pattern_count.values, name='Average Pattern Count'))
fig.add_trace(go.Bar(x=avg_response_count.index, y=avg_response_count.values, name='Average Response Count'))
fig.update_layout(title='Pattern and Response Analysis', xaxis_title='Intents', yaxis_title='Average Count')
fig.show()

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import classification_report
import plotly.graph_objects as go

# Split the dataset into training and testing sets
X = df['patterns']
y = df['tag']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Vectorize the text data using TF-IDF
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# Train a Support Vector Machine (SVM) classifier
model = SVC()
model.fit(X_train_vec, y_train)

# Predict intents for the testing set
y_pred = model.predict(X_test_vec)

# Evaluate the model's performance
report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)

# Convert float values in the report to dictionaries
report = {label: {metric: report[label][metric] for metric in report[label]} for label in report if isinstance(report[label], dict)}

# Extract evaluation metrics
labels = list(report.keys())
evaluation_metrics = ['precision', 'recall', 'f1-score']
metric_scores = {metric: [report[label][metric] for label in labels if label in report] for metric in evaluation_metrics}

# Visualize the model's performance using a Plotly bar plot
fig = go.Figure()
for metric in evaluation_metrics:
    fig.add_trace(go.Bar(name=metric, x=labels, y=metric_scores[metric]))

fig.update_layout(title='Intent Prediction Model Performance',
                  xaxis_title='Intent',
                  yaxis_title='Score',
                  barmode='group')

fig.show()

"""# Prediction Model Deployment

* Once satisfied with the model's performance, deploy the intent prediction model in a chatbot framework.
* Utilize the trained model to predict intents based on user input in real-time.
* Implement an appropriate response generation mechanism to provide relevant and empathetic responses based on the predicted intents.
"""

# Prediction Model Deployment

# A trained SVM model named 'model' and a vectorizer named 'vectorizer'

# Function to predict intents based on user input
def predict_intent(user_input):
    # Vectorize the user input
    user_input_vec = vectorizer.transform([user_input])

    # Predict the intent
    intent = model.predict(user_input_vec)[0]

    return intent

# Function to generate responses based on predicted intents
def generate_response(intent):
    # Implement your logic here to generate appropriate responses based on the predicted intents
    # Find the row in the dataframe that matches the intent
    response_row = df[df['tag'] == intent]
    if not response_row.empty:
        # Randomly select a response from the list of responses for that intent
        responses = response_row['responses'].iloc[0]
        if isinstance(responses, list) and responses:
            response = random.choice(responses)
        elif isinstance(responses, str):
            response = responses
        else:
            response = "I'm here to help. Please let me know how I can assist you."
    else:
        response = "I'm here to help. Please let me know how I can assist you."

    return response

# Example usage
while True:
    # Get user input
    user_input = input("User: ")

    # Predict intent
    intent = predict_intent(user_input)

    # Generate response
    response = generate_response(intent)

    print("Chatbot:", response)

"""##LLM Model Integration"""

from transformers import pipeline

# Load a pre-trained model for text generation
model_name = "distilgpt2"  # You can replace this with a different model
generator = pipeline("text-generation", model=model_name)

import json
import random
import pandas as pd

# Load the intents data (assuming df is already loaded from intents.json)
# with open('/content/intents.json', 'r') as f:
#     data = json.load(f)
# df = pd.DataFrame(data['intents']) # assuming you still have the original df structure

# If you are using the expanded df from cell 7O1zlsAF02Bm, you can directly use that

# Prepare the dataset for LLM training
llm_dataset = []

# If using the original df structure:
# for index, row in df.iterrows():
#     tag = row['tag']
#     patterns = row['patterns']
#     responses = row['responses']
#     for pattern in patterns:
#         # Choose a response (e.g., randomly)
#         response = random.choice(responses)
#         llm_dataset.append({"prompt": pattern, "response": response})

# If using the expanded df structure from cell 7O1zlsAF02Bm:
for index, row in df.iterrows():
    prompt = row['patterns']
    # Assuming 'responses' column in the expanded df is a list of possible responses
    responses = row['responses']
    # Choose a response (e.g., randomly)
    response = random.choice(responses)
    llm_dataset.append({"prompt": prompt, "response": response})


# You can now save llm_dataset to a file or convert it to a pandas DataFrame
# For example, saving to a JSON file:
with open('/content/intents_merged_unique.json', 'w') as f:
    json.dump(llm_dataset, f, indent=2)

# Or convert to a pandas DataFrame
llm_df = pd.DataFrame(llm_dataset)
print(llm_df.head())

import json
import random
import pandas as pd



# If you are using the expanded df from cell 7O1zlsAF02Bm, you can directly use that

# Prepare the dataset for LLM training
llm_dataset = []

# If using the original df structure:
# for index, row in df.iterrows():
#     tag = row['tag']
#     patterns = row['patterns']
#     responses = row['responses']
#     for pattern in patterns:
#         # Choose a response (e.g., randomly)
#         response = random.choice(responses)
#         llm_dataset.append({"prompt": pattern, "response": response})

# If using the expanded df structure from cell 7O1zlsAF02Bm:
for index, row in df.iterrows():
    prompt = row['patterns']
    # Assuming 'responses' column in the expanded df is a list of possible responses
    responses = row['responses']
    # Choose a response (e.g., randomly)
    response = random.choice(responses)
    llm_dataset.append({"prompt": prompt, "response": response})


# You can now save llm_dataset to a file or convert it to a pandas DataFrame
# For example, saving to a JSON file:
with open('llm_training_data.json', 'w') as f:
    json.dump(llm_dataset, f, indent=2)

# Or convert to a pandas DataFrame
llm_df = pd.DataFrame(llm_dataset)
print(llm_df.head())

from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer
import torch
import json
import random
import pandas as pd
from datasets import Dataset


# Load the prepared dataset
with open('llm_training_data.json', 'r') as f:
    llm_dataset = json.load(f)

# Convert the list of dictionaries to a Hugging Face Dataset
hf_dataset = Dataset.from_pandas(pd.DataFrame(llm_dataset))

# Load the tokenizer and model
model_name = "distilgpt2"  # Use the same model name as before
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Add a padding token if the tokenizer doesn't have one
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Preprocess the dataset
def preprocess_function(examples):
    # Combine prompt and response for training
    texts = [f"Prompt: {p}\nResponse: {r}{tokenizer.eos_token}" for p, r in zip(examples['prompt'], examples['response'])]
    tokenized_inputs = tokenizer(texts, truncation=True, padding="max_length", max_length=128) # Adjust max_length as needed

    # For causal language modeling, the labels are the input IDs shifted by one position
    tokenized_inputs["labels"] = tokenized_inputs["input_ids"].copy()
    return tokenized_inputs


tokenized_dataset = hf_dataset.map(preprocess_function, batched=True)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",  # Output directory
    num_train_epochs=3,  # Number of training epochs
    per_device_train_batch_size=4,  # Batch size per device during training
    save_steps=10_000,  # Save model every 10,000 steps
    save_total_limit=2,  # Only keep the latest 2 checkpoints
    logging_dir="./logs",  # Directory for storing logs
    logging_steps=500,
    learning_rate=5e-5,
    weight_decay=0.01,
)

# Create Trainer instance
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

# Start training
trainer.train()

# Save the fine-tuned model
trainer.save_model("./fine-tuned-llm")
tokenizer.save_pretrained("./fine-tuned-llm")

from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer

# Load the fine-tuned model and tokenizer
model_path = "./fine-tuned-llm"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

# Create a text generation pipeline
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Test with a sample prompt
prompt = "Prompt: I am feeling sad"
generated_text = generator(prompt, max_length=50, num_return_sequences=1)[0]['generated_text']

print(generated_text)

"""# Task
Create a Dash interface for the fine-tuned transformer model saved at "fine_tuned_model". The interface should allow users to input text and display the model's generated response.

## Install dash

### Subtask:
Install the necessary libraries for Dash.

[link text](https://)**Reasoning**:
The subtask requires installing the necessary libraries for Dash. This can be done using pip in a code block.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install dash dash-core-components dash-html-components

"""## Import libraries

### Subtask:
Import the required libraries (dash, dash_core_components, dash_html_components, along with the transformers libraries for loading the model).

**Reasoning**:
Import the necessary libraries for building the Dash application and loading the fine-tuned transformer model.
"""

import dash
import dash_core_components as dcc
import dash_html_components as html
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer

"""**Reasoning**:
The previous import statements for `dash_core_components` and `dash_html_components` are deprecated. Update the imports to the current recommended way.


"""

from dash import dcc
from dash import html



"""## Load model and tokenizer

### Subtask:
Load the fine-tuned model and tokenizer from the saved path.

**Reasoning**:
Load the fine-tuned model and tokenizer from the saved path and create a text generation pipeline.
"""

from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer

# Define the path where the fine-tuned model and tokenizer are saved
model_path = "./fine-tuned-llm"

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Load the model
model = AutoModelForCausalLM.from_pretrained(model_path)

# Create a text generation pipeline
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

"""## Create dash app

### Subtask:
Initialize a Dash application.

**Reasoning**:
Initialize a Dash application instance as instructed.
"""

from dash import Dash

app = Dash(__name__)

"""## Define app layout

### Subtask:
Design the layout of the Dash interface with input and output components (e.g., a text area for user input and a display area for the chatbot's response).

**Reasoning**:
Set the layout of the Dash app with a title, a text area for input, and a div for output.
"""

app.layout = html.Div([
    html.H1("Mental Health Chatbot"),
    dcc.Textarea(
        id='user-input',
        placeholder='Enter your message here...',
        style={'width': '100%', 'height': 100}
    ),
    html.Div(id='chatbot-output', children='Chatbot Response:')
])

"""## Define callback function

### Subtask:
Create a callback function that takes user input from the text area, uses the fine-tuned model to generate a response, and updates the output display area.

**Reasoning**:
Define the callback function to handle user input and generate the chatbot's response using the fine-tuned model.
"""

from dash.dependencies import Input, Output

@app.callback(
    Output('chatbot-output', 'children'),
    Input('user-input', 'value')
)
def update_output(user_input):
    if not user_input:
        return 'Chatbot Response:'
    else:
        # Format the input to match the training data format
        prompt = f"Prompt: {user_input}\nResponse:"
        # Generate response using the fine-tuned model
        # Adjust max_length and num_return_sequences as needed
        generated_text = generator(prompt, max_length=len(prompt) + 50, num_return_sequences=1)[0]['generated_text']

        # Extract the response part from the generated text
        # Assuming the model generates text in the format "Prompt: ...\nResponse: ..."
        response_prefix = "Response:"
        response_start = generated_text.find(response_prefix)
        if response_start != -1:
            chatbot_response = generated_text[response_start + len(response_prefix):].strip()
        else:
            # If the expected format is not found, return the whole generated text
            chatbot_response = generated_text.strip()

        return f'Chatbot Response: {chatbot_response}'

"""## Run the dash app

### Subtask:
Add the code to run the Dash application.

**Reasoning**:
Add the code to run the Dash application.
"""

if __name__ == '__main__':
    # Check if running in a Colab environment
    try:
        import google.colab
        IN_COLAB = True
    except ImportError:
        IN_COLAB = False

    if IN_COLAB:
        app.run(debug=True, jupyter_mode="external")
    else:
        app.run(debug=True)

import shutil
import os

source_dir = "./fine-tuned-llm"
destination_dir = "/content/drive/MyDrive/fine-tuned-llm_saved_model"

# Create the destination directory if it doesn't exist
if not os.path.exists(destination_dir):
    os.makedirs(destination_dir)

# Copy the contents of the source directory to the destination directory
shutil.copytree(source_dir, destination_dir, dirs_exist_ok=True)

print(f"Model saved to: {destination_dir}")

"""## Summary:

### Data Analysis Key Findings

*   The necessary libraries for creating a Dash interface, including `dash`, `dash-core-components`, and `dash-html-components`, were successfully installed.
*   The required libraries for building the Dash app and loading the transformer model (`dash`, `dcc`, `html`, `pipeline`, `AutoModelForCausalLM`, and `AutoTokenizer`) were successfully imported.
*   The fine-tuned transformer model and tokenizer were successfully loaded from the specified path ("./fine-tuned-llm"), and a text generation pipeline was created using these components.
*   A Dash application instance was successfully initialized.
*   The layout of the Dash interface was defined, including a title ("Mental Health Chatbot"), a text area for user input (`dcc.Textarea` with id 'user-input'), and a display area for the chatbot's output (`html.Div` with id 'chatbot-output').
*   A callback function `update_output` was successfully defined to handle user input, format it as "Prompt: user input\nResponse:", generate a response using the loaded model, and update the output display area.
*   The code to run the Dash application was added, including logic to handle execution in a Colab environment using `jupyter_mode="external"`.

### Insights or Next Steps

*   The Dash application is now set up to receive user input, process it with the fine-tuned model, and display the generated response. The next step would be to deploy this application to a web server to make it accessible to users.
*   Consider adding error handling within the callback function to gracefully manage potential issues during text generation or unexpected model output formats.

# Task
Help me add my fine-tuned LLM project, including the notebook and the fine-tuned model files located in the "/content/fine-tuned-llm/" directory, to my GitHub and LinkedIn profiles.

## Create a github repository

### Subtask:
Create a new repository on your GitHub account to host the project code.

## Upload project files

### Subtask:
Upload the notebook file (`.ipynb`) and any other necessary files (like the `llm_training_data.json` and the `fine-tuned-llm` model directory) to the GitHub repository.

## Add a readme file

1.   List item

1.   List item

1.   List item

1.   List item
2.   List item


2.   List item


2.   List item


2.   List item



### Subtask:
Create a `README.md` file in your GitHub repository to describe the project, its purpose, how to run it, and the results.

## Showcase on linkedin

### Subtask:
Update your LinkedIn profile to include this project in the "Featured" or "Projects" section.

## Summary:

### Data Analysis Key Findings

*   The entire process of adding the fine-tuned LLM project to GitHub and LinkedIn profiles could not be completed automatically using the provided tools.
*   Creating a GitHub repository, uploading project files, and adding a README file are manual steps that require user interaction with the GitHub website.
*   Showcasing the project on LinkedIn also requires manual updates to the user's profile on the LinkedIn website.

### Insights or Next Steps

*   All steps in the requested process need to be performed manually by the user on the respective platforms (GitHub and LinkedIn).
*   The user should proceed by creating the GitHub repository, uploading the project files and a README, and then adding the project details to their LinkedIn profile manually.
"""